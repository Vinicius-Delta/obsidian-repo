The device needed in such situations, a hand-held thermometer, has risen from a specialist item to a common sight.

A branch of Artificial Intelligence known as “computer vision” focuses on automated image labeling. Most computer vision systems were trained on data sets that contained very few images of hand-held thermometers. As a result, they cannot label the device correctly.

In [an experiment that became viral](https://twitter.com/nicolaskb/status/1244921742486917120 "https://twitter.com/nicolaskb/status/1244921742486917120") on Twitter, AlgorithmWatch showed that Google Vision Cloud, a computer vision service, labeled an image of a dark-skinned individual holding a thermometer “gun” while a similar image with a light-skinned individual was labeled “electronic device”. A subsequent experiment showed that the image of a dark-skinned hand holding a thermometer was labelled “gun” and that the same image with a salmon-colored overlay on the hand was enough for the computer to label it “monocular”.

![[Pasted image 20211023144458.png]]

Google since updated its algorithm. As of 6 April, it does not return a “gun” label anymore.

Agathe Balayn, a PhD candidate at the Delft University of Technology on the topic of bias in automated systems, concurs. She tested several images in Google's service and came to the conclusion that the example might be "a case of inaccuracy without a statistical bias." In the absence of more rigorous testing, it is impossible to say that the system is biased, she wrote.

Because most of these systems are similar to Google Vision Cloud, “they could easily have the same biases”, Ms Raji wrote. As a result, dark-skinned individuals are more likely to be flagged as dangerous even if they hold an object as harmless as a hand-held thermometer.

----


Because dark-skinned individuals probably featured much more often in scenes depicting violence in the training data set, a computer making automated inferences on an image of a dark-skinned hand is much more likely to label it with a term from the lexical field of violence.

Other computer vision systems show similar biases. In December, Facebook [refused](https://www.instagram.com/p/B5HJrglJjHq/ "https://www.instagram.com/p/B5HJrglJjHq/") to let an Instagram user from Brazil advertise a picture, arguing that it contained weapons. In fact, it was a drawing of a boy and Formula One driver Lewis Hamilton. Both characters had dark skins.

--------

### Real-world consequences

Labeling errors could have consequences in the physical world. Deborah Raji, a tech fellow at New York University’s AI Now Institute and a specialist in computer vision, wrote in an email that, in the United States, weapon recognition tools are used in schools, concerts halls, apartment complexes and supermarkets. In Europe, automated surveillance deployed by some police forces probably use it as well. Because most of these systems are similar to Google Vision Cloud, “they could easily have the same biases”, Ms Raji wrote. As a result, dark-skinned individuals are more likely to be flagged as dangerous even if they hold an object as harmless as a hand-held thermometer.

Nakeema Stefflbauer, founder and CEO of FrauenLoop, a community of technologists with a focus on inclusivity, wrote in an email that bias in computer vision software would “definitely” impact the lives of dark-skinned individuals. Because the rate of mis-identification is consistently higher for women and dark-skinned people, the spread of computer vision for surveillance would disproportionately affect them, she added.

Referring to the examples of Ousmane Bah, a teenager who was [wrongly accused of theft](https://www.bloomberg.com/news/articles/2019-04-22/apple-face-recognition-blamed-by-new-york-teen-for-false-arrest "https://www.bloomberg.com/news/articles/2019-04-22/apple-face-recognition-blamed-by-new-york-teen-for-false-arrest") at an Apple Store because of faulty face recognition, and of Amara K. Majeed, who was [wrongly accused](https://www.bostonglobe.com/metro/2019/04/28/brown-student-mistaken-identified-sri-lanka-bombings-suspect/0hP2YwyYi4qrCEdxKZCpZM/story.html "https://www.bostonglobe.com/metro/2019/04/28/brown-student-mistaken-identified-sri-lanka-bombings-suspect/0hP2YwyYi4qrCEdxKZCpZM/story.html") of taking part in the 2019 Sri Lanka bombings after her face was misidentified, Ms Stefflbauer foresees that, absent effective regulation, whole groups could end up avoiding certain buildings or neighborhoods. Individuals could face _de facto_ restrictions in their movements, were biased computer vision to be more widely deployed, she added.




------
KAYSER-BRIL, N. **Google apologizes after its Vision AI produced racist results****AlgorithmWatch**, 7 abr. 2020. Disponível em: <[https://algorithmwatch.org/en/google-vision-racism/](https://algorithmwatch.org/en/google-vision-racism/)>. Acesso em: 23 out. 2021