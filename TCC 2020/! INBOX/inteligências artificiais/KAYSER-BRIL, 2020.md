The device needed in such situations, a hand-held thermometer, has risen from a specialist item to a common sight.

A branch of Artificial Intelligence known as “computer vision” focuses on automated image labeling. Most computer vision systems were trained on data sets that contained very few images of hand-held thermometers. As a result, they cannot label the device correctly.

In [an experiment that became viral](https://twitter.com/nicolaskb/status/1244921742486917120 "https://twitter.com/nicolaskb/status/1244921742486917120") on Twitter, AlgorithmWatch showed that Google Vision Cloud, a computer vision service, labeled an image of a dark-skinned individual holding a thermometer “gun” while a similar image with a light-skinned individual was labeled “electronic device”. A subsequent experiment showed that the image of a dark-skinned hand holding a thermometer was labelled “gun” and that the same image with a salmon-colored overlay on the hand was enough for the computer to label it “monocular”.

![[Pasted image 20211023144458.png]]

Google since updated its algorithm. As of 6 April, it does not return a “gun” label anymore.

Agathe Balayn, a PhD candidate at the Delft University of Technology on the topic of bias in automated systems, concurs. She tested several images in Google's service and came to the conclusion that the example might be "a case of inaccuracy without a statistical bias." In the absence of more rigorous testing, it is impossible to say that the system is biased, she wrote.



----


Because dark-skinned individuals probably featured much more often in scenes depicting violence in the training data set, a computer making automated inferences on an image of a dark-skinned hand is much more likely to label it with a term from the lexical field of violence.

Other computer vision systems show similar biases. In December, Facebook [refused](https://www.instagram.com/p/B5HJrglJjHq/ "https://www.instagram.com/p/B5HJrglJjHq/") to let an Instagram user from Brazil advertise a picture, arguing that it contained weapons. In fact, it was a drawing of a boy and Formula One driver Lewis Hamilton. Both characters had dark skins.

------
KAYSER-BRIL, N. **Google apologizes after its Vision AI produced racist results****AlgorithmWatch**, 7 abr. 2020. Disponível em: <[https://algorithmwatch.org/en/google-vision-racism/](https://algorithmwatch.org/en/google-vision-racism/)>. Acesso em: 23 out. 2021